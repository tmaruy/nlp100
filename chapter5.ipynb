{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36464bitbaseconda1d0c2e3be8214c02b0fb731b7870def2",
   "display_name": "Python 3.6.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 言語処理100本ノック 2020 第５章\n",
    "\n",
    "Reference: https://nlp100.github.io/ja/ch05.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cabocha -f1 neko.txt > neko.txt.cabocha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40. 係り受け解析結果の読み込み（形態素）\n",
    "\n",
    "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文をMorphオブジェクトのリストとして表現し，3文目の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morph():\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "    def __str__(self):\n",
    "        return(self.surface)\n",
    "    def summary(self):\n",
    "        return(\"{}\\t{}\\t{}\\t{}\".format(self.surface, self.base, self.pos, self.pos1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"neko.txt.cabocha\", \"r\", encoding=\"utf-8\") as fi:\n",
    "    lines = fi.readlines()\n",
    "    s = [] # sentence\n",
    "    s_s = [] # list of sentence\n",
    "    for line in lines:\n",
    "        if re.match(\"^EOS\", line):\n",
    "            if len(s) != 0: s_s.append(s)\n",
    "            s = []\n",
    "        elif not (re.match(\"^\\u3000\", line) or re.match(\"^\\*\", line)):\n",
    "            word, info = line.strip(\"\\n\").split(\"\\t\")\n",
    "            info_list = info.split(\",\")\n",
    "            pos, pos1 = info_list[:2]\n",
    "            if(len(info_list) > 6): base = info_list[6]\n",
    "            else: base = \"\"\n",
    "            s.append(Morph(word, base, pos, pos1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "名前\t名前\t名詞\t一般\nは\tは\t助詞\t係助詞\nまだ\tまだ\t副詞\t助詞類接続\n無い\t無い\t形容詞\t自立\n。\t。\t記号\t句点\n"
    }
   ],
   "source": [
    "for m in s_s[2]: print(m.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "\n",
    "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk():\n",
    "    def __init__(self, morphs, id, dst, srcs):\n",
    "        self.morphs = morphs\n",
    "        self.id = int(id)\n",
    "        self.dst = int(dst)\n",
    "        self.srcs = srcs\n",
    "    def __str__(self):\n",
    "        return \"\".join([str(morph) for morph in self.morphs])\n",
    "    def summary(self):\n",
    "        words = \" \".join([str(morph) for morph in self.morphs])\n",
    "        str_src = \"\".join([str(src) for src in self.srcs])\n",
    "        return \"{}\\t{}\\t{}\\t\".format(self.id, words, self.dst, str_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"neko.txt.cabocha\", \"r\", encoding=\"utf-8\") as fi:\n",
    "    lines = fi.readlines()\n",
    "    morphs = [] # list of morphs\n",
    "    s = [] # sentence\n",
    "    s_s = [] # list of sentence\n",
    "    for line in lines:\n",
    "        if re.match(\"^EOS\", line):\n",
    "            if len(s) != 0:\n",
    "                s.append(Chunk(morphs, id, dst, []))\n",
    "                s_s.append(s)\n",
    "            s = []\n",
    "            morphs = []\n",
    "        elif re.match(\"^\\*\", line):\n",
    "            if len(morphs) != 0:\n",
    "                s.append(Chunk(morphs, id, dst, []))\n",
    "            _, id, dst, _, _ = line.strip(\"\\n\").split(\" \")\n",
    "            dst = dst.strip(\"D\")\n",
    "            morphs = []\n",
    "        else:\n",
    "            word, info = line.strip(\"\\n\").split(\"\\t\")\n",
    "            info_list = info.split(\",\")\n",
    "            pos, pos1 = info_list[:2]\n",
    "            if(len(info_list) > 6): base = info_list[6]\n",
    "            else: base = \"\"\n",
    "            morphs.append(Morph(word, base, pos, pos1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in s_s:\n",
    "    for c in s:\n",
    "        if c.dst > 0: s[c.dst].srcs.append(c.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\tしかし\t9\t\n1\tその\t2\t\n2\t当時 は\t5\t\n3\t何 という\t4\t\n4\t考 も\t5\t\n5\tなかっ た から\t9\t\n6\t別段\t7\t\n7\t恐し\t9\t\n8\tいとも\t9\t\n9\t思わ なかっ た 。\t-1\t\n"
    }
   ],
   "source": [
    "for c in s_s[7]:\n",
    "    print(c.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 42. 係り元と係り先の文節の表示\n",
    "\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "しかし\t思わなかった\nその\t当時は\n当時は\tなかったから\n何という\t考も\n考も\tなかったから\nなかったから\t思わなかった\n別段\t恐し\n恐し\t思わなかった\nいとも\t思わなかった\n思わなかった\t\n"
    }
   ],
   "source": [
    "s = s_s[7]\n",
    "for c in s:\n",
    "    start = str(c).strip(\"。、\")\n",
    "    if c.dst > 0: dest = str(s[c.dst]).strip(\"。、\")\n",
    "    else: dest = \"\"\n",
    "    print(\"{}\\t{}\".format(start, dest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "　どこで\t生れたか\n見当が\tつかぬ\n"
    }
   ],
   "source": [
    "s = s_s[2]\n",
    "for c_start in s:\n",
    "    if c_start.dst > 0: c_dest = s[c_start.dst]\n",
    "    else: continue\n",
    "    if (\"名詞\" in [m.pos for m in c_start.morphs]) and (\"動詞\" in [m.pos for m in c_dest.morphs]):\n",
    "        start = str(c_start).strip(\"。、\")\n",
    "        dest = str(c_dest).strip(\"。、\")\n",
    "        print(\"{}\\t{}\".format(start, dest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 44. 係り受け木の可視化\n",
    "\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<graphviz.dot.Digraph at 0x1a27d7add8>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"344pt\" height=\"260pt\"\n viewBox=\"0.00 0.00 344.39 260.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-256 340.3945,-256 340.3945,4 -4,4\"/>\n<!-- しかし -->\n<g id=\"node1\" class=\"node\">\n<title>しかし</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"31.1972\" cy=\"-90\" rx=\"31.3957\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"31.1972\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">しかし</text>\n</g>\n<!-- 思わなかった -->\n<g id=\"node10\" class=\"node\">\n<title>思わなかった</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"180.1972\" cy=\"-18\" rx=\"51.9908\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"180.1972\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">思わなかった</text>\n</g>\n<!-- しかし&#45;&gt;思わなかった -->\n<g id=\"edge1\" class=\"edge\">\n<title>しかし&#45;&gt;思わなかった</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M55.1367,-78.432C78.1694,-67.302 113.4102,-50.2729 140.6191,-37.125\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"142.394,-40.1546 149.8751,-32.6523 139.3484,-33.8519 142.394,-40.1546\"/>\n</g>\n<!-- その -->\n<g id=\"node2\" class=\"node\">\n<title>その</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"64.1972\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"64.1972\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">その</text>\n</g>\n<!-- 当時は -->\n<g id=\"node3\" class=\"node\">\n<title>当時は</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"67.1972\" cy=\"-162\" rx=\"31.3957\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"67.1972\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">当時は</text>\n</g>\n<!-- その&#45;&gt;当時は -->\n<g id=\"edge2\" class=\"edge\">\n<title>その&#45;&gt;当時は</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M64.9543,-215.8314C65.2751,-208.131 65.6566,-198.9743 66.0132,-190.4166\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"69.5106,-190.5503 66.43,-180.4133 62.5166,-190.2589 69.5106,-190.5503\"/>\n</g>\n<!-- なかったから -->\n<g id=\"node6\" class=\"node\">\n<title>なかったから</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"132.1972\" cy=\"-90\" rx=\"51.9908\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"132.1972\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">なかったから</text>\n</g>\n<!-- 当時は&#45;&gt;なかったから -->\n<g id=\"edge3\" class=\"edge\">\n<title>当時は&#45;&gt;なかったから</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M81.6228,-146.0209C89.9084,-136.843 100.4921,-125.1196 109.834,-114.7716\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"112.574,-116.9595 116.6772,-107.1915 107.3782,-112.2688 112.574,-116.9595\"/>\n</g>\n<!-- 何という -->\n<g id=\"node4\" class=\"node\">\n<title>何という</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"147.1972\" cy=\"-234\" rx=\"38.1938\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"147.1972\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">何という</text>\n</g>\n<!-- 考も -->\n<g id=\"node5\" class=\"node\">\n<title>考も</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"145.1972\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"145.1972\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">考も</text>\n</g>\n<!-- 何という&#45;&gt;考も -->\n<g id=\"edge4\" class=\"edge\">\n<title>何という&#45;&gt;考も</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M146.6925,-215.8314C146.4786,-208.131 146.2243,-198.9743 145.9866,-190.4166\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"149.4851,-190.3122 145.7087,-180.4133 142.4878,-190.5066 149.4851,-190.3122\"/>\n</g>\n<!-- 考も&#45;&gt;なかったから -->\n<g id=\"edge5\" class=\"edge\">\n<title>考も&#45;&gt;なかったから</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M141.9168,-143.8314C140.5264,-136.131 138.8732,-126.9743 137.328,-118.4166\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"140.7431,-117.6322 135.5218,-108.4133 133.8544,-118.8761 140.7431,-117.6322\"/>\n</g>\n<!-- なかったから&#45;&gt;思わなかった -->\n<g id=\"edge6\" class=\"edge\">\n<title>なかったから&#45;&gt;思わなかった</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M144.0624,-72.2022C149.7007,-63.7448 156.5516,-53.4685 162.7659,-44.147\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"165.8337,-45.855 168.4685,-35.593 160.0093,-41.9721 165.8337,-45.855\"/>\n</g>\n<!-- 別段 -->\n<g id=\"node7\" class=\"node\">\n<title>別段</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"229.1972\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"229.1972\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">別段</text>\n</g>\n<!-- 恐し -->\n<g id=\"node8\" class=\"node\">\n<title>恐し</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"229.1972\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"229.1972\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">恐し</text>\n</g>\n<!-- 別段&#45;&gt;恐し -->\n<g id=\"edge7\" class=\"edge\">\n<title>別段&#45;&gt;恐し</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M229.1972,-143.8314C229.1972,-136.131 229.1972,-126.9743 229.1972,-118.4166\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"232.6973,-118.4132 229.1972,-108.4133 225.6973,-118.4133 232.6973,-118.4132\"/>\n</g>\n<!-- 恐し&#45;&gt;思わなかった -->\n<g id=\"edge8\" class=\"edge\">\n<title>恐し&#45;&gt;思わなかった</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M217.8325,-73.3008C211.9449,-64.6496 204.6205,-53.8873 198.0021,-44.1623\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"200.7038,-41.9111 192.184,-35.6132 194.9168,-45.8495 200.7038,-41.9111\"/>\n</g>\n<!-- いとも -->\n<g id=\"node9\" class=\"node\">\n<title>いとも</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"305.1972\" cy=\"-90\" rx=\"31.3957\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"305.1972\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">いとも</text>\n</g>\n<!-- いとも&#45;&gt;思わなかった -->\n<g id=\"edge9\" class=\"edge\">\n<title>いとも&#45;&gt;思わなかった</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M282.8497,-77.1278C264.2932,-66.4393 237.5473,-51.0337 216.0084,-38.6272\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"217.5475,-35.4747 207.1353,-33.5163 214.0536,-41.5404 217.5475,-35.4747\"/>\n</g>\n</g>\n</svg>\n"
     },
     "metadata": {},
     "execution_count": 171
    }
   ],
   "source": [
    "s = s_s[7]\n",
    "dg = Digraph() # graph\n",
    "\n",
    "for c in s: \n",
    "    dg.node(str(c).strip(\"。、\"))\n",
    "\n",
    "for c in s:\n",
    "    if c.dst > 0: \n",
    "        dg.edge(str(c).strip(\"。、\"), str(s[c.dst]).strip(\"。、\"))\n",
    "dg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 45. 動詞の格パターンの抽出\n",
    "\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "* 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "* 述語に係る助詞を格とする\n",
    "* 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "    始める  で\n",
    "    見る    は を\n",
    "\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "* コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "* 「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, s in enumerate(s_s):\n",
    "    for c in s:\n",
    "        verb = \"\"\n",
    "        pps = [] # 助詞\n",
    "        for m in c.morphs:\n",
    "            if m.pos == \"動詞\":\n",
    "                verb = m.base\n",
    "                break\n",
    "        if verb == \"\": continue\n",
    "        #\n",
    "        for src in c.srcs:\n",
    "            for m in s[src].morphs[::-1]:\n",
    "                if m.pos == \"助詞\":\n",
    "                    pps.append(m.base)\n",
    "                    break\n",
    "        if len(pps) > 0: results.append(\"{}\\t{}\".format(verb, \" \".join(pps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "生れる\tで\nつく\tか が\n泣く\tで\nする\tて は\n始める\tで\n見る\tは を\n聞く\tで\n捕える\tを\n煮る\tて\n食う\tて\n"
    }
   ],
   "source": [
    "for i in results[:10]: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array(['云う\\tと', 'する\\tを', '思う\\tと', 'ある\\tが', 'なる\\tに', 'する\\tに', '見る\\tて',\n        'する\\tと', 'する\\tが', '見る\\tを'], dtype='<U22'),\n array([704, 452, 333, 202, 199, 188, 175, 159, 117,  98]))"
     },
     "metadata": {},
     "execution_count": 239
    }
   ],
   "source": [
    "tmp = np.unique(results, return_counts=True)\n",
    "idx = np.argsort(tmp[1])[::-1][:10]\n",
    "tmp[0][idx], tmp[1][idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 46. 動詞の格フレーム情報の抽出\n",
    "\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "* 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "* 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "    始める  で      ここで\n",
    "    見る    は を   吾輩は ものを"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, s in enumerate(s_s):\n",
    "    for c in s:\n",
    "        verb = \"\"\n",
    "        pps = [] # 助詞\n",
    "        chunks = [] # 文節\n",
    "        for m in c.morphs:\n",
    "            if m.pos == \"動詞\":\n",
    "                verb = m.base\n",
    "                break\n",
    "        if verb == \"\": continue\n",
    "        #\n",
    "        for src in c.srcs:\n",
    "            for m in s[src].morphs[::-1]:\n",
    "                if m.pos == \"助詞\":\n",
    "                    pps.append(m.base)\n",
    "                    chunks.append(str(s[src]).strip(\"、。\"))\n",
    "                    break\n",
    "        if len(pps) > 0: results.append(\"{}\\t{}\\t{}\".format(verb, \" \".join(pps), \" \".join(chunks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "生れる\tで\t　どこで\nつく\tか が\t生れたか 見当が\n泣く\tで\t所で\nする\tて は\t泣いて いた事だけは\n始める\tで\tここで\n見る\tは を\t吾輩は ものを\n聞く\tで\tあとで\n捕える\tを\t我々を\n煮る\tて\t捕えて\n食う\tて\t煮て\n"
    }
   ],
   "source": [
    "for i in results[:10]: print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 47. 機能動詞構文のマイニング\n",
    "\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "\n",
    "* 「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "* 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "* 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "* 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
    "\n",
    "例えば「別段くるにも及ばんさと、主人は手紙に返事をする。」という文から，以下の出力が得られるはずである．\n",
    "\n",
    "    返事をする      と に は        及ばんさと 手紙に 主人は\n",
    "\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "* コーパス中で頻出する述語（サ変接続名詞+を+動詞）\n",
    "* コーパス中で頻出する述語と助詞パターン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, s in enumerate(s_s):\n",
    "    for c in s:\n",
    "        if len(c.morphs) != 2: continue\n",
    "        if (c.morphs[0].pos1 != \"サ変接続\") or (c.morphs[1].base != \"を\"): continue\n",
    "        #\n",
    "        verb = \"\"\n",
    "        for m in s[c.dst].morphs:\n",
    "            if m.pos == \"動詞\":\n",
    "                verb = m.base\n",
    "                break\n",
    "        if verb == \"\": continue\n",
    "        verb = str(c) + verb\n",
    "        #\n",
    "        pps = [] # 助詞\n",
    "        chunks = [] # 文節\n",
    "        for src in s[c.dst].srcs:\n",
    "            if s[src] == c: continue\n",
    "            for m in s[src].morphs[::-1]:\n",
    "                if m.pos == \"助詞\":\n",
    "                    pps.append(m.base)\n",
    "                    chunks.append(str(s[src]).strip(\"、。\"))\n",
    "                    break\n",
    "        if len(pps) > 0: results.append(\"{}\\t{}\\t{}\".format(verb, \" \".join(pps), \" \".join(chunks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "決心をする\tと\tこうと\n返報をする\tんで\t偸んで\n昼寝をする\tが\t彼が\n迫害を加える\tて\t追い廻して\n投書をする\tて へ\tやって ほととぎすへ\n話をする\tに\t時に\n昼寝をする\tて\t出て\n欠伸をする\tから て て\tなったから して 押し出して\n報道をする\tに\t耳に\n御馳走を食う\tと\t見ると\n"
    }
   ],
   "source": [
    "for i in results[:10]: print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 48. 名詞から根へのパスの抽出\n",
    "\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "\n",
    "* 各文節は（表層形の）形態素列で表現する\n",
    "* パスの開始文節から終了文節に至るまで，各文節の表現を” -> “で連結する\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という文（neko.txt.cabochaの5文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "    吾輩は -> 見た\n",
    "    ここで -> 始めて -> 人間という -> ものを -> 見た\n",
    "    人間という -> ものを -> 見た\n",
    "    ものを -> 見た\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(idx, s):\n",
    "    if s[idx].dst > 0:\n",
    "        return [str(s[idx]).strip(\"、。\")] + get_path(s[idx].dst, s)\n",
    "    else:\n",
    "        return [str(s[idx]).strip(\"、。\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = s_s[4]\n",
    "results = []\n",
    "for i, c in enumerate(s):\n",
    "    if not \"名詞\" in [m.pos for m in c.morphs]: continue\n",
    "    results.append(\" -> \".join(get_path(i, s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['吾輩は -> 見た',\n 'ここで -> 始めて -> 人間という -> ものを -> 見た',\n '人間という -> ものを -> 見た',\n 'ものを -> 見た']"
     },
     "metadata": {},
     "execution_count": 288
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49. 名詞間の係り受けパスの抽出\n",
    "\n",
    "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がiとj（i<j）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "\n",
    "* 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を” -> “で連結して表現する\n",
    "* 文節iとjに含まれる名詞句はそれぞれ，XとYに置換する\n",
    "\n",
    "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "\n",
    "* 文節iから構文木の根に至る経路上に文節jが存在する場合: 文節iから文節jのパスを表示\n",
    "* 上記以外で，文節iと文節jから構文木の根に至る経路上で共通の文節kで交わる場合: 文節iから文節kに至る直前のパスと文節jから文節kに至る直前までのパス，文節kの内容を” | “で連結して表示\n",
    "\n",
    "例えば，「吾輩はここで始めて人間というものを見た。」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "    Xは | Yで -> 始めて -> 人間という -> ものを | 見た\n",
    "    Xは | Yという -> ものを | 見た\n",
    "    Xは | Yを | 見た\n",
    "    Xで -> 始めて -> Y\n",
    "    Xで -> 始めて -> 人間という -> Y\n",
    "    Xという -> Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}