{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36464bitbaseconda1d0c2e3be8214c02b0fb731b7870def2",
   "display_name": "Python 3.6.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 言語処理100本ノック 2020 第８章\n",
    "\n",
    "Reference: https://nlp100.github.io/ja/ch08.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70. 単語ベクトルの和による特徴量\n",
    "\n",
    "問題50で構築した学習データ，検証データ，評価データを行列・ベクトルに変換したい．例えば，学習データについて，すべての事例xiの特徴ベクトルxiを並べた行列Xと，正解ラベルを並べた行列（ベクトル）Yを作成したい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                               TITLE CATEGORY\n0      Shia Labeouf - Shia Labeouf In Rehab - Report        e\n1  UPDATE 1-China bans use of Microsoft's Windows...        t\n2      Mischa Barton Stuns In Flattering Black Dress        e\n3  Aereo's Legal Battles Rest on the Meaning of '...        e\n4        Ohio Mumps Outbreak Up To 56 Reported Cases        m",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TITLE</th>\n      <th>CATEGORY</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Shia Labeouf - Shia Labeouf In Rehab - Report</td>\n      <td>e</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>UPDATE 1-China bans use of Microsoft's Windows...</td>\n      <td>t</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Mischa Barton Stuns In Flattering Black Dress</td>\n      <td>e</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Aereo's Legal Battles Rest on the Meaning of '...</td>\n      <td>e</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ohio Mumps Outbreak Up To 56 Reported Cases</td>\n      <td>m</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "train = pd.read_csv(\"NewsAggregatorDataset/train.txt\", sep=\"\\t\")\n",
    "test = pd.read_csv(\"NewsAggregatorDataset/test.txt\", sep=\"\\t\")\n",
    "valid = pd.read_csv(\"NewsAggregatorDataset/valid.txt\", sep=\"\\t\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dictionary(19395 unique tokens: ['In', 'Labeouf', 'Rehab', 'Report', 'Shia']...)\nDictionary(1894 unique tokens: ['In', 'Report', 'Shia', \"'s\", '1-China']...)\n"
    }
   ],
   "source": [
    "# Tokenize\n",
    "tokens = [word_tokenize(text) for text in train.TITLE]\n",
    "# Remove (i) numbers, (ii) single-character words\n",
    "tokens = [[t for t in token if (not t.isnumeric()) and (len(t) > 1)] for token in tokens]\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "print(dictionary)\n",
    "\n",
    "# Filter out words that occur less than 10 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    In  Report  Shia   's  1-China\n0  1.0     1.0   1.0  0.0      0.0\n1  0.0     0.0   0.0  1.0      1.0\n2  1.0     0.0   0.0  0.0      0.0\n3  0.0     0.0   0.0  1.0      0.0\n4  0.0     0.0   0.0  0.0      0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>In</th>\n      <th>Report</th>\n      <th>Shia</th>\n      <th>'s</th>\n      <th>1-China</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "def vectorize(texts, dictionary):\n",
    "    # Tokenize\n",
    "    tokens = [word_tokenize(text) for text in texts]\n",
    "    # Vectorize\n",
    "    corpus = [dictionary.doc2bow(t) for t in tokens]\n",
    "    # \n",
    "    mat = pd.DataFrame(np.zeros([len(tokens), len(dictionary)]), \n",
    "                       columns=dictionary.token2id.keys())\n",
    "    for i,c_s in enumerate(corpus):\n",
    "        idx = [c[0] for c in c_s]\n",
    "        mat.iloc[i, idx] = 1\n",
    "    return(mat)\n",
    "\n",
    "x_train = vectorize(train.TITLE, dictionary)\n",
    "x_test = vectorize(test.TITLE, dictionary)\n",
    "x_valid = vectorize(valid.TITLE, dictionary)\n",
    "x_train.iloc[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[2, 1, 2, 2, 3]"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "#b = business, t = science and technology, e = entertainment, m = health\n",
    "y_dict = {\"b\":0, \"t\":1, \"e\":2, \"m\":3}\n",
    "y_train = [y_dict[c] for c in train.CATEGORY]\n",
    "y_test = [y_dict[c] for c in test.CATEGORY]\n",
    "y_valid = [y_dict[c] for c in valid.CATEGORY]\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 71. 単層ニューラルネットワークによる予測\n",
    "\n",
    "問題70で保存した行列を読み込み，学習データについて以下の計算を実行せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizers\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123) # torch用の乱数シード\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(input_dim, output_dim)\n",
    "        self.layers = [self.l1]\n",
    "        nn.init.uniform_(self.l1.weight, -1, 1)\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TestModel(x_train.shape[1], 4).to(device)\n",
    "x_ = torch.Tensor(np.array(x_train)).to(device)\n",
    "y_ = torch.Tensor(y_train).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 2.5144,  0.3729,  0.7250, -1.3400],\n        [-2.4293, -1.1343,  0.9164, -1.8876],\n        [ 2.4407,  0.7129, -0.8651, -1.5174],\n        ...,\n        [-1.5226, -0.1192,  1.4111, -0.0818],\n        [ 0.5885,  1.3574,  1.5553,  1.5798],\n        [ 2.0930, -2.3336, -0.0579, -1.4801]], grad_fn=<AddmmBackward>)"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "model.forward(x_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 72. 損失と勾配の計算\n",
    "\n",
    "学習データの事例x1と事例集合x1,x2,x3,x4に対して，クロスエントロピー損失と，行列Wに対する勾配を計算せよ．なお，ある事例xiに対して損失は次式で計算される．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "def compute_loss(t, y):\n",
    "    return criterion(y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(2.2120, grad_fn=<NllLossBackward>)"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "preds = model.forward(x_)\n",
    "loss = compute_loss(y_.long(), preds) # long型にしないとエラーが出る\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 73. 確率的勾配降下法による学習\n",
    "\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，行列Wを学習せよ．なお，学習は適当な基準で終了させればよい（例えば「100エポックで終了」など）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = TestModel(x_train.shape[1], 4).to(device)\n",
    "\n",
    "# Optimmizer\n",
    "optimizer = optimizers.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Dataset\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = torch.Tensor(np.array(x)).to(device)\n",
    "        self.y = torch.Tensor(y).long().to(device)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        x_out = self.x[idx]\n",
    "        y_out = self.y[idx]\n",
    "        if self.transform:\n",
    "            x_out = self.transform(x_out)\n",
    "        return x_out, y_out\n",
    "train_dataset = TestDataset(x_train, y_train)\n",
    "\n",
    "# DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, t):\n",
    "    model.train()\n",
    "    preds = model(x)\n",
    "    loss = compute_loss(t, preds)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, preds   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 1, loss: 1.63, acc: 0.387\nepoch: 11, loss: 1.01, acc: 0.632\nepoch: 21, loss: 0.831, acc: 0.693\nepoch: 31, loss: 0.723, acc: 0.733\nepoch: 41, loss: 0.648, acc: 0.765\nepoch: 51, loss: 0.594, acc: 0.788\nepoch: 61, loss: 0.552, acc: 0.807\nepoch: 71, loss: 0.519, acc: 0.818\nepoch: 81, loss: 0.492, acc: 0.831\nepoch: 91, loss: 0.469, acc: 0.839\nepoch: 100, loss: 0.451, acc: 0.846\n"
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    for (x, t) in train_dataloader:\n",
    "        x, t = x.to(device), t.to(device)\n",
    "        loss, preds = train_step(x, t)\n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy_score(t.tolist(), preds.argmax(dim=1).tolist())\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "    if(epoch % 10 == 0 or epoch == epochs-1):\n",
    "        print('epoch: {}, loss: {:.3}, acc: {:.3f}'.format(epoch+1, train_loss, train_acc ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 74. 正解率の計測\n",
    "\n",
    "問題73で求めた行列を用いて学習データおよび評価データの事例を分類したとき，その正解率をそれぞれ求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}